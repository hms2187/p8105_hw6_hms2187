---
title: "hw6"
author: "Henry Stoddard"
date: "11/24/2020"
output: github_document
---
```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)
library(readxl)
library(modelr)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
set.seed(1)
```
Load in libraries and settings

## Problem 1
```{r}
homicide_df =
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest" ~ 0,
      disposition == "Closed by arrest" ~ 1)
    ) %>% 
  filter(victim_race %in% c("White", "Black"),
         city_state != "Tulsa, AL") %>% 
  select(city_state, resolution, victim_age, victim_race, victim_sex)

```

starting with one city
```{r}
baltimore_df =
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")

glm(resolution ~ victim_age + victim_race + victim_sex, data = baltimore_df, family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96*std.error),
    CI_upper = exp(estimate + 1.96*std.error)
  ) %>% 
  select(term, OR, starts_with("CI")) %>% 
  knitr::kable(digits = 3)
```
Now we will nest to get df for each city and then map across each city, then tidy results to produce new dataframe with desired outcomes.

```{r}
model_results_df =
homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = map(.x = data, ~glm(resolution ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96*std.error),
    CI_upper = exp(estimate + 1.96*std.error)
  ) %>% 
  select(term, OR, starts_with("CI"))
```

in each city, are arrests resolved differently by sex of victim?
```{r}
model_results_df %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
The plot seems to show a pretty wide variation in ORs. The lowest OR is in New York, the highest OR is in Albuquerque.

## Problem 2

Loading in and cleaning data
```{r}
bw_df =
  read.csv("./data/birthweight.csv") %>% 
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    malform = as.factor(malform),
    mrace = as.factor(mrace)
  )
summary(bw_df)
any(is.na(bw_df))
```
Categorical vars have been turned into factors from numeric. There appears to be no missing data.

To create a model, I googled things that predict birthweight and found a webpage on Webmd that referenced a study by Duke researchers. Their model included fetal sex, stage of pregnancy, mother's height and weight, weight gain during pregnancy, and number of other children. Though their variables were slightly different, we have variables that are close enough to mimic such a model.
Here is the model for birthweight:
```{r}
model_1 =
  lm(bwt ~ babysex + gaweeks + mheight + delwt + wtgain + parity, bw_df)
broom::glance(model_1)
broom::tidy(model_1) %>% knitr::kable(digits = 3)

plot_distrib =
  bw_df %>% 
  add_residuals(model_1) %>% 
  ggplot(aes(x=resid)) +
  geom_density()
plot_distrib

plot_1 =
  bw_df %>% 
  add_predictions(model_1) %>% 
  add_residuals(model_1) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point()
plot_1
```

Residuals look normally distributed. Plotting residuals against predictions looks pretty good and there are no obvious outliers.

Now making two additional models, plus looking at their summaries.
```{r}
model_2 =
  lm(bwt ~ blength + gaweeks, bw_df)
broom::tidy(model_2) %>% knitr::kable(digits = 3)

model_3 =
  lm(bwt ~ blength*bhead*babysex, bw_df)
broom::tidy(model_3) %>% knitr::kable(digits = 3)
```

Trying to understand model fit using CV. 

```{r}
cv_df = 
  crossv_mc(bw_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```

See if I can fit the models to the splits ...

```{r}
cv_df = 
  cv_df %>% 
  mutate(
    model_1 = map(.x = train, ~lm(bwt ~ babysex + gaweeks + mheight + delwt + wtgain + parity, data = .x)),
    model_2 = map(.x = train, ~ lm(bwt ~ blength + gaweeks, data = .x)),
    model_3 = map(.x = train, ~lm(bwt ~ blength*bhead*babysex, data = .x))
  ) %>% 
  mutate(
    rmse_1 = map2_dbl(.x = model_1, .y = test, ~rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(.x = model_2, .y = test, ~rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(.x = model_3, .y = test, ~rmse(model = .x, data = .y))
  )
```

Violin plot of RMSEs

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()
```

Looks like model 3 is the winner, with a lower RMSE than 2 and well lower than model 1 (sorry Duke researchers/WebMD).

## Problem 3
Importing data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Now running bootstraps to make two different samples. One (called r2) collects r^2 values for each bootstrap, then makes a density plot of the estimates, then creates a table that shows the 95% CI.
The second sample (called beta) does the same thing but for the beta 0 and beta 1 values. It then manipulates those values (multiplies them together and takes the log of the product), plots the distribution of the manipulated estimate, and creates a 95% CI.
```{r}
weather_model =
  lm(tmax ~ tmin, data = weather_df)

r2 =
weather_df %>% 
  bootstrap(5000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm(tmax ~ tmin, data = .x)), 
    results1 = map(models, broom::glance)
  ) %>% 
  select(strap_number, results1) %>% 
  unnest(results1) %>% 
  select(r.squared) 

r2_plot =
  r2 %>% 
  ggplot(aes(x=r.squared)) +
  geom_density()
r2_plot

r2_ci =
  r2 %>% 
  summarize(
    ci_lower = quantile(r.squared, 0.025),
    ci_upper = quantile(r.squared, 0.975)
  ) %>% 
  knitr::kable(digits = 3)
r2_ci
```
The density plot shows mostly normal distribution of r^2 values, centering around 0.91. The confidence limits range from around 0.90 to 0.93, indicating strong, positive correlation between tmax and tmin. This makes sense, given that I would expect a hot day with high tmax to also have a high tmin, and same vice versa for cold days.

```{r}
beta =
weather_df %>% 
  bootstrap(5000, id = "strap_number") %>% 
  mutate(
    models = map(.x = strap, ~lm(tmax ~ tmin, data = .x)), 
    results1 = map(models, broom::tidy)
  ) %>% 
  select(strap_number, results1) %>% 
  unnest(results1) %>% 
  select(strap_number, term, estimate) %>% 
  pivot_wider(names_from = term, values_from = estimate) %>% 
  group_by(strap_number) %>% 
  summarize(
    new_estimate = log(`(Intercept)`*tmin)
  )

beta_plot =
  beta %>% 
  ggplot(aes(x=new_estimate)) +
  geom_density()
beta_plot

beta_ci =
  beta %>% 
  summarize(
    ci_lower = quantile(new_estimate, 0.025),
    ci_upper = quantile(new_estimate, 0.975)
  ) %>% 
  knitr::kable(digits = 3)
beta_ci
```
The density plot for the "new estimate" (which is equal to the log of the product of b0 and b1) appears normal and centered at 2.02. The CI for this estimate ranges from 1.97 to 2.06.